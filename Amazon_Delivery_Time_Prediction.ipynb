{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "1dmO-2ZchlEz",
        "aBKYA5Huh0pR",
        "x71ZqKXriCWQ",
        "h1EGLwPg557Z",
        "Rdlrm8oZcKDS",
        "OzPh8PqpcMUR",
        "booCJCLlmPnH",
        "lzmlYsEPmbnC",
        "XzKXYnzzw2Ad",
        "bOTzaopEw8oA"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AaryanPriyadarshi/Amazon-Delivery-Time-Prediction/blob/main/Amazon_Delivery_Time_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Amazon Delivery Time Prediction\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA / Regression / Forecasting\n",
        "\n",
        "##### **Contribution**    - Aaryan Priyadarshi\n",
        "\n",
        "##### **Domain**          - E-Commerce & Logistics\n",
        "\n",
        "#####**Goal**             - Predict delivery times based on traffic, weather, distance, and delivery agent data."
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project focuses on predicting Amazon delivery times using machine learning to improve logistics efficiency and customer satisfaction. The dataset contains key operational parameters such as traffic conditions, weather, distance, vehicle type, and agent performance metrics.\n",
        "After cleaning and validating the data, several engineered features were added — including distance between store and customer (via the Haversine formula) and time-based variables like month, weekday, and order hour.\n",
        "Exploratory Data Analysis (EDA) revealed strong relationships between distance, traffic intensity, and delivery time, which were further validated using hypothesis testing.\n",
        "Machine learning models — including Random Forest Regressor and XGBoost Regressor — were implemented and evaluated based on RMSE, MAE, and R² scores.\n",
        "Among these, XGBoost delivered the best performance, demonstrating its ability to capture non-linear patterns effectively.\n",
        "Finally, feature importance and SHAP analysis provided valuable interpretability, identifying distance, traffic conditions, and agent ratings as the most influential factors.\n",
        "This end-to-end solution not only predicts delivery duration accurately but also provides data-driven insights for route optimization and workforce planning in e-commerce logistics."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/AaryanPriyadarshi/Amazon-Delivery-Time-Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accurately predicting delivery time is a critical challenge in e-commerce logistics. Factors such as traffic congestion, weather variations, route distance, and delivery agent performance contribute to unpredictable delivery durations.\n",
        "The problem this project addresses is:\n",
        "\n",
        ">How can we use data-driven modeling to accurately predict delivery times and identify the operational factors that most affect them?\n",
        "\n",
        "By building predictive models and analyzing the underlying drivers of delivery delays, this project aims to help Amazon and similar companies optimize delivery schedules, allocate resources efficiently, and enhance the customer experience through reliable delivery time estimates."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Import Libraries**\n",
        "\n",
        "Imported necessary libraries for:\n",
        "\n",
        "- **Data handling:** `pandas`, `numpy`  \n",
        "- **Data visualization:** `matplotlib`, `seaborn`, `plotly`  \n",
        "- **Machine learning:** `sklearn`, `xgboost`  \n",
        "- **Statistical testing:** `scipy`  \n",
        "- **Explainability:** `shap`  \n",
        "- **Logging, warnings, and reproducibility**\n"
      ],
      "metadata": {
        "id": "1dmO-2ZchlEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import warnings, logging, random, time, os\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Others\n",
        "from math import radians, sin, cos, sqrt, atan2\n",
        "from scipy import stats\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "logging.info(\"Libraries imported and configurations set successfully.\")\n"
      ],
      "metadata": {
        "id": "GgrsuRw0hrfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Dataset Loading**\n",
        "\n",
        "Mounted Google Drive and loaded the **Amazon Delivery dataset** stored in Drive.  \n",
        "\n",
        "Verified that the dataset is accessible, correctly formatted, and loaded into a pandas DataFrame.  \n",
        "Previewed dataset shape and the first few rows for confirmation.\n"
      ],
      "metadata": {
        "id": "aBKYA5Huh0pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the dataset path inside Google Drive\n",
        "DATA_PATH = \"/content/drive/MyDrive/Amazon Delivery Time Prediction/amazon_delivery.csv\"\n",
        "\n",
        "def load_dataset(path):\n",
        "    \"\"\"Load dataset from Drive and verify existence.\"\"\"\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File not found at: {path}\")\n",
        "    df = pd.read_csv(path)\n",
        "    logging.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "# Load dataset\n",
        "df = load_dataset(DATA_PATH)\n",
        "\n",
        "# Display basic information\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Dataset First View\n",
        "\n",
        "- Preview dataset with `.head()`\n",
        "- Check basic info with `.info()` and `.describe()`\n",
        "- Count missing values\n"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def explore_data(df):\n",
        "    \"\"\"Perform initial data checks.\"\"\"\n",
        "    logging.info(f\"Shape: {df.shape}\")\n",
        "    logging.info(f\"Missing values:\\n{df.isnull().sum()}\")\n",
        "    logging.info(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "    return df.isnull().sum()\n",
        "\n",
        "explore_data(df)\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Input Validation\n",
        "\n",
        "Checked if the required columns exist in the dataset:  \n",
        "\n",
        "- Order, delivery, and location fields  \n",
        "- Verified presence of key identifiers for feature engineering  \n",
        "\n",
        "Ensures the dataset is complete and properly structured before further analysis.\n"
      ],
      "metadata": {
        "id": "K4uUKy7Msjf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "required_columns = [\n",
        "    \"Order_ID\", \"Delivery_Time\",\n",
        "    \"Store_Latitude\", \"Store_Longitude\",\n",
        "    \"Drop_Latitude\", \"Drop_Longitude\",\n",
        "    \"Order_Date\"\n",
        "]\n",
        "\n",
        "missing = [col for col in required_columns if col not in df.columns]\n",
        "if missing:\n",
        "    logging.warning(f\"Missing essential columns: {missing}\")\n",
        "else:\n",
        "    logging.info(\"All required columns found.\")\n"
      ],
      "metadata": {
        "id": "yQh9geKDs6qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Feature Engineering\n",
        "\n",
        "Created new informative features to enhance model performance:\n",
        "\n",
        "- **Distance (km):** Calculated using the Haversine formula between store and customer locations  \n",
        "- **Temporal features:** Extracted month, weekday, weekend flag, and hour from order date/time  \n",
        "- **Interaction features:** Combined distance with traffic intensity for added predictive insight  \n",
        "\n",
        "These transformations allow the model to better understand geographical and time-based trends.\n"
      ],
      "metadata": {
        "id": "h1EGLwPg557Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import radians, sin, cos, sqrt, atan2\n",
        "\n",
        "def haversine_km(lat1, lon1, lat2, lon2):\n",
        "    \"\"\"Calculate distance between two coordinates in kilometers.\"\"\"\n",
        "    R = 6371.0  # Radius of Earth in km\n",
        "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
        "    dlat = lat2 - lat1\n",
        "    dlon = lon2 - lon1\n",
        "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
        "    return R * 2 * atan2(sqrt(a), sqrt(1 - a))\n",
        "\n",
        "def add_features(df):\n",
        "    \"\"\"Add distance, temporal, and interaction-based features.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # ✅ Distance Feature\n",
        "        df['distance_km'] = df.apply(lambda r: haversine_km(\n",
        "            r['Store_Latitude'], r['Store_Longitude'],\n",
        "            r['Drop_Latitude'], r['Drop_Longitude']\n",
        "        ), axis=1)\n",
        "        logging.info(\"Distance feature created successfully.\")\n",
        "    except KeyError as e:\n",
        "        logging.warning(f\"Missing column for distance calculation: {e}\")\n",
        "\n",
        "    # ✅ Convert and extract date/time features\n",
        "    if 'Order_Date' in df.columns:\n",
        "        df['Order_Date'] = pd.to_datetime(df['Order_Date'], errors='coerce')\n",
        "        df['Order_Month'] = df['Order_Date'].dt.month\n",
        "        df['Order_Weekday'] = df['Order_Date'].dt.weekday\n",
        "        df['Is_Weekend'] = df['Order_Weekday'].isin([5, 6]).astype(int)\n",
        "\n",
        "    if 'Order_Time' in df.columns:\n",
        "        df['Order_Hour'] = pd.to_datetime(df['Order_Time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
        "\n",
        "    # ✅ Interaction Feature (distance × traffic)\n",
        "    if 'Traffic' in df.columns:\n",
        "        df['Traffic_code'] = LabelEncoder().fit_transform(df['Traffic'].astype(str))\n",
        "        if 'distance_km' in df.columns:\n",
        "            df['distance_x_traffic'] = df['distance_km'] * df['Traffic_code']\n",
        "\n",
        "    logging.info(\"Feature engineering completed successfully.\")\n",
        "    return df\n",
        "\n",
        "# Apply the feature engineering\n",
        "df = add_features(df)\n",
        "\n",
        "# Display first few rows\n",
        "display(df.head())\n"
      ],
      "metadata": {
        "id": "qrjbT35bliKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Feature Verification\n",
        "\n",
        "Verified that new engineered features were successfully created and contain valid data.  \n",
        "This step ensures all transformations — distance, temporal, and interaction features — exist, have correct data types, and minimal missing values.\n"
      ],
      "metadata": {
        "id": "pevInEblrrro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List new engineered features\n",
        "engineered_features = [\n",
        "    'distance_km',\n",
        "    'Order_Month',\n",
        "    'Order_Weekday',\n",
        "    'Is_Weekend',\n",
        "    'Order_Hour',\n",
        "    'Traffic_code',\n",
        "    'distance_x_traffic'\n",
        "]\n",
        "\n",
        "print(\"Checking engineered features...\\n\")\n",
        "\n",
        "# Check which features exist\n",
        "existing = [col for col in engineered_features if col in df.columns]\n",
        "missing = [col for col in engineered_features if col not in df.columns]\n",
        "\n",
        "print(f\"Existing features: {existing}\")\n",
        "if missing:\n",
        "    print(f\"Missing features: {missing}\\n\")\n",
        "else:\n",
        "    print(\"All engineered features are present.\\n\")\n",
        "\n",
        "# Show summary statistics for numeric engineered features\n",
        "display(df[existing].describe())\n",
        "\n",
        "# Check for nulls and data types\n",
        "print(\"\\nNull Values:\")\n",
        "display(df[existing].isnull().sum())\n",
        "\n",
        "print(\"\\nData Types:\")\n",
        "display(df[existing].dtypes)\n"
      ],
      "metadata": {
        "id": "FEqg1n7frvJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Data Visualization\n",
        "\n",
        "Explored and visualized data patterns using different plots:\n",
        "\n",
        "- **Delivery Time Distribution:** To view time variability and skewness  \n",
        "- **Distance vs Delivery Time:** To check the correlation between distance and delay  \n",
        "- **Delivery Time by Traffic Condition:** To see how traffic affects delivery duration  \n",
        "- **Correlation Heatmap:** To understand numeric relationships between variables  \n",
        "\n",
        "Helps identify key features influencing delivery performance.\n"
      ],
      "metadata": {
        "id": "Rdlrm8oZcKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delivery Time Distribution\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.histplot(df['Delivery_Time'].dropna(), kde=True)\n",
        "plt.title(\"Delivery Time Distribution\")\n",
        "plt.xlabel(\"Hours\")\n",
        "plt.show()\n",
        "\n",
        "# Distance vs Delivery Time\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.scatterplot(x='distance_km', y='Delivery_Time', data=df, alpha=0.5)\n",
        "plt.title(\"Distance vs Delivery Time\")\n",
        "plt.show()\n",
        "\n",
        "# Delivery Time by Traffic\n",
        "if 'Traffic' in df.columns:\n",
        "    plt.figure(figsize=(10,5))\n",
        "    sns.boxplot(x='Traffic', y='Delivery_Time', data=df)\n",
        "    plt.title(\"Delivery Time by Traffic Condition\")\n",
        "    plt.show()\n",
        "\n",
        "# Correlation Heatmap\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.heatmap(df.select_dtypes('number').corr(), annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Z1NOAyO-l1Ki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Hypothesis Testing\n",
        "\n",
        "Formulated and tested the hypothesis:  \n",
        "\n",
        "> “Weekend deliveries take longer than weekday deliveries.”  \n",
        "\n",
        "Used an **independent t-test** to compare mean delivery times between weekend and weekday groups.  \n",
        "This validates if delivery delays significantly differ based on the day of the week.\n"
      ],
      "metadata": {
        "id": "OzPh8PqpcMUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weekend = df[df['Is_Weekend'] == 1]['Delivery_Time'].dropna()\n",
        "weekday = df[df['Is_Weekend'] == 0]['Delivery_Time'].dropna()\n",
        "\n",
        "t_stat, p_val = stats.ttest_ind(weekend, weekday, equal_var=False)\n",
        "print(f\"T-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n",
        "\n",
        "if p_val < 0.05:\n",
        "    print(\"Reject Null Hypothesis: Weekend and weekday delivery times differ significantly.\")\n",
        "else:\n",
        "    print(\"Fail to Reject Null Hypothesis: No significant difference.\")\n"
      ],
      "metadata": {
        "id": "tTG-ogc6rgit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. Data Preprocessing\n",
        "\n",
        "Prepared the dataset for machine learning by:\n",
        "\n",
        "- **Encoding categorical variables** using Label Encoding  \n",
        "- **Removing irrelevant identifiers** (e.g., Order ID)  \n",
        "- **Handling missing values** using median imputation  \n",
        "- **Splitting data** into 80% training and 20% testing sets  \n",
        "- **Scaling features** using StandardScaler for model compatibility  \n",
        "\n",
        "Ensures all features are clean, numerical, and properly scaled.\n"
      ],
      "metadata": {
        "id": "booCJCLlmPnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_split(df, target='Delivery_Time', test_size=0.2, seed=42):\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Drop irrelevant ID-like columns\n",
        "    drop_cols = ['Order_ID', 'Order_Date', 'Pickup_Time']  # Drop datetime & identifiers\n",
        "    df_copy.drop(columns=drop_cols, errors='ignore', inplace=True)\n",
        "\n",
        "    # Encode categorical columns\n",
        "    cat_cols = df_copy.select_dtypes(include='object').columns\n",
        "    le = LabelEncoder()\n",
        "    for c in cat_cols:\n",
        "        df_copy[c] = le.fit_transform(df_copy[c].astype(str))\n",
        "\n",
        "    # Handle missing values for numeric features\n",
        "    for col in df_copy.select_dtypes(include=np.number).columns:\n",
        "        df_copy[col].fillna(df_copy[col].median(), inplace=True)\n",
        "\n",
        "    # Define X and y\n",
        "    X = df_copy.drop(columns=[target], errors='ignore')\n",
        "    y = df_copy[target]\n",
        "\n",
        "    # Keep only numeric columns for training\n",
        "    X = X.select_dtypes(include=[np.number])\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    logging.info(\"Preprocessing completed successfully.\")\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, X.columns\n",
        "\n",
        "# Run preprocessing\n",
        "X_train_scaled, X_test_scaled, y_train, y_test, feature_names = preprocess_and_split(df)\n"
      ],
      "metadata": {
        "id": "evBCaIzHmSRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Model Implementation\n",
        "\n",
        "Implemented and trained multiple regression models:\n",
        "\n",
        "- **Random Forest Regressor**  \n",
        "- **XGBoost Regressor**  \n",
        "\n",
        "Each model was evaluated using the following metrics:  \n",
        "- Root Mean Squared Error (**RMSE**)  \n",
        "- Mean Absolute Error (**MAE**)  \n",
        "- Coefficient of Determination (**R²**)  \n",
        "\n",
        "This step determines the best-performing model for delivery time prediction.\n"
      ],
      "metadata": {
        "id": "lzmlYsEPmbnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
        "    start = time.time()\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "    mae = mean_absolute_error(y_test, preds)\n",
        "    r2 = r2_score(y_test, preds)\n",
        "    duration = time.time() - start\n",
        "    logging.info(f\"{name} — RMSE: {rmse:.3f}, MAE: {mae:.3f}, R²: {r2:.3f}, Time: {duration:.2f}s\")\n",
        "    return {\"Model\": name, \"RMSE\": rmse, \"MAE\": mae, \"R²\": r2}\n",
        "\n",
        "models = [\n",
        "    (\"Random Forest\", RandomForestRegressor(n_estimators=200, random_state=SEED)),\n",
        "    (\"XGBoost\", XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=SEED))\n",
        "]\n",
        "\n",
        "results = [evaluate_model(n, m, X_train_scaled, X_test_scaled, y_train, y_test) for n, m in models]\n",
        "results_df = pd.DataFrame(results).sort_values(\"RMSE\")\n",
        "display(results_df)\n"
      ],
      "metadata": {
        "id": "eLCYqdj9mcyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Model Comparison\n",
        "\n",
        "Compared model performance visually using bar plots to highlight differences in RMSE, MAE, and R².  \n",
        "\n",
        "Helps quickly identify the model that performs best across accuracy and error metrics.\n"
      ],
      "metadata": {
        "id": "XzKXYnzzw2Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.set_index(\"Model\")[[\"RMSE\", \"MAE\", \"R²\"]].plot(kind='bar', figsize=(8,5), colormap='viridis')\n",
        "plt.title(\"Model Performance Comparison\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qD7ES2G9w3DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Feature Importance\n",
        "\n",
        "Assessed which features most strongly influence delivery time prediction:  \n",
        "\n",
        "- Used **feature importance scores** from tree-based models  \n",
        "- Applied **SHAP explainability** (if available) for interpretability  \n",
        "\n",
        "These insights help understand the relative impact of distance, traffic, and timing on delivery duration.\n"
      ],
      "metadata": {
        "id": "bOTzaopEw8oA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "best_model_name = results_df.iloc[0][\"Model\"]\n",
        "best_model = RandomForestRegressor(n_estimators=200, random_state=SEED) if best_model_name == \"Random Forest\" else XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=SEED)\n",
        "best_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "fi = pd.DataFrame({\"Feature\": feature_names, \"Importance\": best_model.feature_importances_}).sort_values(\"Importance\", ascending=False).head(20)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=\"Importance\", y=\"Feature\", data=fi, palette=\"coolwarm\")\n",
        "plt.title(f\"{best_model_name} - Top 20 Important Features\")\n",
        "plt.show()\n",
        "\n",
        "if SHAP_AVAILABLE:\n",
        "    explainer = shap.Explainer(best_model, X_train_scaled)\n",
        "    shap_values = explainer(X_test_scaled)\n",
        "    shap.summary_plot(shap_values, X_test_scaled, feature_names=feature_names, show=True)\n"
      ],
      "metadata": {
        "id": "B0RVnFbSw9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **12. Conclusion**\n",
        "\n",
        "- Delivery time is mainly affected by **distance**, **traffic conditions**, and **order timing**.  \n",
        "- **XGBoost** achieved the highest predictive accuracy among models tested.  \n",
        "- Feature importance confirms that geographical and temporal factors dominate prediction performance.  \n",
        "\n",
        "### **Future Enhancements**\n",
        "- Integrate **real-time traffic and weather data**  \n",
        "- Apply **Bayesian hyperparameter tuning** for optimization  \n",
        "- Deploy the model via **Streamlit or Flask** for real-world usability\n",
        "\n"
      ],
      "metadata": {
        "id": "seslXGUhx1_D"
      }
    }
  ]
}